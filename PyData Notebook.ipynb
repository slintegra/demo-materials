{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='img/logo.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\"> Energy Production Optimization based on Market </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [The Problem Domain](#The-Problem-Domain)\n",
    "\n",
    "2. [Required libraries](#Required-libraries)\n",
    "\n",
    "3. [Data Exploration](#Data-Exploration)\n",
    "\n",
    "4. [Machine Learning Model Building](#Machine-Learning-Model-Building)\n",
    "\n",
    "5. [Machine Learning Model Evaluations](#Machine-Learning-Model-Evaluations)\n",
    "\n",
    "6. [Market Analysis](#Market-Analysis)\n",
    "\n",
    "7. [Acknowledgements](#Acknowledgements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "In this notebook, we will showcase the process of machine learning. Topics that would be introduced in this notebook includes EDA, feature engineering, model training, hyperparameter tuning, and model validation.  \n",
    "\n",
    "### The Problem Domain - Electricity Generation\n",
    "\n",
    "We will be demonstrating machine learning in a common usecase of predicting power generation. The prediction of electrical power output is often important in making economical and operation decision of a power plant. In this usecase, we will be predicting the production of a combined cycle powerplant, and subsequentially use the predicted results for examplary market action recommendations. \n",
    "\n",
    "The dataset that is used for this usecase are:\n",
    "- Combined cycle powerplant data from [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant)\n",
    "- Pool price forecast data from [AESO](http://ets.aeso.ca/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required libraries\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "This notebook uses several Python packages that come standard with the Anaconda Python distribution. The primary libraries that we'll be using are:\n",
    "\n",
    "*Data operations and model building*\n",
    "\n",
    "* **numpy**: A library for fast numerical array processing\n",
    "* **pandas**: A library for working with dataframes\n",
    "* **pandas-profiling**: A library working with pandas dataframe with which provides quick and basic exploratory data analysis.\n",
    "* **scipy**: A library for statistical fitting, modeling, and analysis\n",
    "* **sklearn** - A library for machine learning model building\n",
    "\n",
    "*Graphing and visualizations*\n",
    "* **plotly** - A library for plotting interactive graphs\n",
    "* **matplotlib & mpl_toolkit** - A library for visualization and graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data operations and ML model \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV, GridSearchCV, cross_validate, KFold\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import svm, tree\n",
    "\n",
    "# graphing and visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Powerplant Data\n",
    "\n",
    "This dataset contains 9568 data points collected from a combined cycle powerplant over 6 years, where the power plant was set to work with full load. Hour data were collected for average ambient temperature (Celcius), ambient pressure (Hg), relative humidity (%), exhaust vacuum (millibar) and net electrical energy output (MW). For demonstration purpose, has been modified with a mockup datetime variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please do not change the file path if wish to use default data\n",
    "df = pd.read_csv('data/powerplant.csv', header = 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conduct the initial data exploration we will use the **pandas-profiling** library for generating basic insights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(df, title=\"Powerplant Data EDA Report\")\n",
    "profile.to_widgets()\n",
    "\n",
    "# Uncomment the following line to render the report into a HTML file\n",
    "# profile.to_file(\"Explore_powerplant_output.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <b>TIPS</b>\n",
    ">\n",
    "> The report shows no signification need to wrangle data. Specifically, there are no missing data and no outliers which are common data problems in real-life situations. We can continue with our analysis with the uploaded dataset. \n",
    "> \n",
    "> However, there are high correlations among the features (Temperature & Exhaust Vaccum Pressure), which could lead to colinearity problems in some machine learning models. These could potentially be work that needs to be done in the feature engineering and model optimization steps in a machine learning model building project. \n",
    ">\n",
    "> We also see some correlations between our features and our target (EnergyOutput). Thus, another possible solution to this usecase may be the use of regression analysis. Which in some situations that can be beneficial for their reduced (generally) computation requirements in comparison to machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Market Data\n",
    "\n",
    "We have retrieved pool price forecast data from AESO, for the time of our market analysis, which is from 2020-12-06 to 2020-12-12. The data contains forecast and actual value of the pool price ($/MWH) and the Alberta internal load (AIL), which is the demand of energy (MW) at the hour. The forecast of pool price is made two hours prior to the listed time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "# Please do not change the file path if wish to use default data\n",
    "aeso_df = pd.read_csv('data/ActualForecastWMRQHReportServlet.csv', thousands=\",\")\n",
    "aeso_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'Date' column contains the date and the hour. However, this dataset starts the count of hour at 1, but most datetime objects in python starts the hour at 0. Thus, some data parsing is needed for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Split the hour from the date into a separate column\n",
    "aeso_df[['Date','Hour']] = aeso_df['Date'].str.split(' ',expand=True)\n",
    "\n",
    "# 2. Subtract 1 from the hour and convert type from integer to string\n",
    "aeso_df.Hour = aeso_df.Hour.astype(int)-1\n",
    "aeso_df.Hour = aeso_df.Hour.astype(str).str.zfill(2)\n",
    "\n",
    "# 3. Add the two columns back together and convert string into datetime data type\n",
    "aeso_df['Datetime'] = pd.to_datetime(aeso_df.Date+\" \"+aeso_df.Hour, format = \"%m/%d/%Y %H\")\n",
    "\n",
    "# 4. Drop unnecessary columns\n",
    "aeso_df.drop(['Date', 'Hour'], axis=1, inplace=True)\n",
    "\n",
    "aeso_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment the following lines to render pandas_profiling report for the AESO dataset\n",
    "\n",
    "# aeso_profile = ProfileReport(aeso_df, title=\"AESO EDA Report\")\n",
    "# aeso_profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "In machine learning it is important to split your data and reserve a portion for testing and validations. The common practice is to split the dataset into 3 portions: Train 60%, Test 20%, and Validation 20%.\n",
    "\n",
    "> <b> TIPS </b>\n",
    ">\n",
    "> Generally, the train and test data would be used in the repetitive process to build, improve and optimize your model. In the process of this model buidling process, this likelihood that you'll induce **optimization bias** increases as you repeat the process. In simple terms, optimization bias is encountering a model that gives you very good performance on your development dataset by random chance. However, the model performs poorly on other data. This is a part of a common problem in machine learning  known as **overfitting**. \n",
    ">\n",
    "> Thus it is important to always leave a portion of the data to be untouched until the very end as a final validation of your model. Validating your model on data that it has never seen before, is also known as the <b>Golden Rule</b>. \n",
    "\n",
    "However, in this notebook, we will use the cross validation method to evaluate models. Thus, we will only split out the last 20% (about 3 months worth) in the dataset to serve as our validation data, and 80% of the data serve as the working data in building the model. The purpose of this is to imitate the senario where the model was trained on past data, but needs to predict values in the future. \n",
    "\n",
    "For the purpose of this notebook's demo, we also reserve the last week worth of data and remove the \"HourlyEnergyOutputMW\" target column to imitate real-time hourly data that is to be passed into the model for prediction. Then subsequently passed to a market analysis model to give action recommendation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept: K - Fold Cross Validation\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into feature engineering and hyperparameter tuning, we will need to introduce the concept of k-folds cross validation. Which is an important concept in model validations and hyperparameter tuning. K-folds cross validation is a common evaluation method in machine learning when working with a limited sample. The intuition behind k-fold cross validation is to use the most out of your sample, and that every single sample can contribute to the training of the model. \n",
    "\n",
    "How cross validation (CV) work is depicted in the following schematic retrieved from http://ethen8181.github.io/machine-learning/model_selection/model_selection.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='img/kfolds.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$k$ is the number partitions that you choose to split your data into. It is common to work with 5-folds, or 10-folds cross validations. \n",
    "\n",
    "The general process works as the following:\n",
    "1. Reserve a partition to serve as the test set\n",
    "2. Train model on the remaining partitions\n",
    "3. Evaluation model on the reserved partition and record score\n",
    "4. Discard model\n",
    "\n",
    "\n",
    "If we choose to use a 5-folds cross validation. The above process will be repeated five times. Each time a different partition is reserved to use as the test set. By using cross validation, we can use our data to the largest extent such that:\n",
    "- Each observation contribute to the model training process\n",
    "- Each observation have the chance to serve as an 'unseen' observation\n",
    "- The iterative process helps with managing the bias-variance tradeoff in machine learning\n",
    "\n",
    "> **NOTE**\n",
    ">\n",
    "> Cross validation (CV) is often used in hyperparameter tuning to evaluation model performance. Each set of hyperparameters is put into cross validation evaluation, then the set of hyperparameters that returns the highest cv score would then be taken to retrain the model on the entire dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data\n",
    "train = df[df['Datetime'] < '2020-10-01'].copy() # Train and Test\n",
    "valid = df[(df['Datetime'] >= '2020-10-01') & (df['Datetime'] < '2020-12-06')].copy() # Validation\n",
    "forecast = df[df['Datetime'] >= '2020-12-06'].copy() # Reserved for market analysis\n",
    "\n",
    "# Split target(y) away from the features (x)\n",
    "x_train = train.drop('HourlyEnergyOutputMW', axis=1)\n",
    "y_train = train[['HourlyEnergyOutputMW']]\n",
    "\n",
    "x_valid = valid.drop('HourlyEnergyOutputMW', axis=1)\n",
    "y_valid = valid[['HourlyEnergyOutputMW']]\n",
    "\n",
    "# Drop data in the market analysis to imitate \"real-time\" data from powerplant sensors\n",
    "forecast.drop(\"HourlyEnergyOutputMW\", axis=1, inplace=True)\n",
    "\n",
    "# Print size of train, test, and valid\n",
    "print('Training size:\\t\\t', x_train.shape[0])\n",
    "print('Validation size:\\t', x_valid.shape[0])\n",
    "print('Forecast size:\\t\\t', forecast.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal in feature engineering is to transform your data into a form that machine learning algorithms performs well with. The work that falls under this umbrella includes (but not limited to): imputations, outliers handling, binning, features splitting, features aggregating, etc. \n",
    "\n",
    "In here we recommend reading a great article by Emre Rençberoğlu, [Fundamental Techniques of Feature Engineering for Machine Learning](https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114) which introduce many common feature engineering techniques. \n",
    "\n",
    "It is important to know what transformations and engineering techniques suit well for your dataset. The dataset that we have does not have any missing values or outliers. Most of the features that we have does not have large ranges where we might consider applying log transformations. However, if I am considering KNN models and other distance models, I may consider applying scaling on the numerical values. Additionally, if you believe the interval of time may play a role in affecting energy output, you may extracts the month and date from the datetime.\n",
    "\n",
    "An important concept in feature engineering is to apply the **same transformations** fitted from the training to test datasets. \n",
    "\n",
    "For example, normalization requires the max and min of the feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training \"AmbientPressure\" max/min: {}/{}'.format(x_train.AmbientPressureMillibar.min(), x_train.AmbientPressureMillibar.max()))\n",
    "print('Validation \"AmbientPressure\" max/min: {}/{}'.format(x_valid.AmbientPressureMillibar.min(), x_valid.AmbientPressureMillibar.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply a normalization with min=993.31 and max=1033.3 to the testing feature, even though the actual min/max of this feature in the validation dataset is different. \n",
    "\n",
    "> **TIP**\n",
    ">\n",
    "> Always apply the same transformations fitted from the trainin dataset to the testing and validation data. \n",
    "\n",
    "In the code cell below, we built a sample class object that defines transformation functions, the transformation pipeline, and tracks the fitted transformation parameters. The code below is for demonstration, the Scikit-Learn package also offer many powerful feature engineering functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformPipe():\n",
    "    def __init__(self, x_train):\n",
    "        self.x_train = x_train\n",
    "        self.transform_feature = [] # Tracks the list of features to apply transforamtions\n",
    "        self.transform_params = [] # Tracks the parameters fitted to each transformation\n",
    "        self.transformations = [] # Tracks the transformations applied to the data\n",
    "        self.drop_feature = [] # Tracks the list of features to drop\n",
    "        self.x_train_transformed = None\n",
    "    \n",
    "    # params = (x_min, x_max)\n",
    "    def normalization(self, x, params=None, drop_original=True):\n",
    "        x_name = x.columns[0]\n",
    "        if params is None:\n",
    "            x_min=x.min()\n",
    "            x_max = x.max()\n",
    "            self.transformations.append(self.normalization)\n",
    "            self.transform_params.append((x_min, x_max))\n",
    "            self.transform_feature.append(x_name)\n",
    "            if drop_original:\n",
    "                self.drop_feature.append(x_name)\n",
    "        else:\n",
    "            x_min, x_max = params\n",
    "        x_norm = (x - x_min)/(x_max-x_min)\n",
    "        x_norm = x_norm.rename(columns = {x_name: 'Normed'+x_name})\n",
    "        return(x_norm)\n",
    "    \n",
    "    # params = (x_mu, x_sigma)\n",
    "    def standardization(self, x, params=None, drop_original=True):\n",
    "        x_name = x.columns[0]\n",
    "        if params is None:\n",
    "            x_mu = x.mean()\n",
    "            x_sigma = x.std()\n",
    "            self.transformations.append(self.standardization)\n",
    "            self.transform_params.append((x_mu, x_sigma))\n",
    "            self.transform_feature.append(x_name)\n",
    "            if drop_original:\n",
    "                self.drop_feature.append(x_name)\n",
    "        else:\n",
    "            x_mu, x_sigma = params\n",
    "        x_stand = (x-x_mu)/x_sigma\n",
    "        x_stand = x_stand.rename(columns = {x_name: 'Standard'+x_name})\n",
    "        return(x_stand)\n",
    "    \n",
    "    def extractMonth(self, x, fit=True, drop_original=True):\n",
    "        x_name = x.columns[0]\n",
    "        if fit:\n",
    "            self.transformations.append(self.extractMonth)\n",
    "            self.transform_params.append(False)\n",
    "            self.transform_feature.append(x_name)\n",
    "            if drop_original:\n",
    "                self.drop_feature.append(x_name)\n",
    "        x_datetime = pd.to_datetime(x[x_name])\n",
    "        x = x.assign(Month=x_datetime.dt.month)\n",
    "        return(x[['Month']])\n",
    "    \n",
    "    # Categorize date into beginning of month (1st - 10th), middle of month (11th-20th), end of month (20th-end)\n",
    "    # and apply one-hot-encoding\n",
    "    def groupDate(self, x, fit=True, drop_original=True):\n",
    "        x_name = x.columns[0]\n",
    "        if fit:\n",
    "            self.transformations.append(self.groupDate)\n",
    "            self.transform_params.append(False)\n",
    "            self.transform_feature.append(x_name)\n",
    "            if drop_original:\n",
    "                self.drop_feature.append(x_name)\n",
    "        x_datetime = pd.to_datetime(x[x_name])\n",
    "        x_date = x_datetime.dt.day\n",
    "        x = x.assign(TimeofMonth = np.where(x_date<10, 'early', np.where(x_date < 20, 'middle', 'late')))\n",
    "        enc = OneHotEncoder(categories=[['early', 'middle', 'late']], handle_unknown='ignore')\n",
    "        \n",
    "        x_encoded = pd.DataFrame(enc.fit_transform(x[['TimeofMonth']]).toarray(), columns=['MonthBegin', 'MonthMiddle', 'MonthLate'], index=x.index)\n",
    "        return(x_encoded)\n",
    "    \n",
    "    # Define the transformation pipeline here on the training dataset\n",
    "    def fit(self):\n",
    "        x = self.x_train.copy()\n",
    "        x_pipe = [x]\n",
    "        \n",
    "        # Pipe Transformations\n",
    "        x_pipe.append(self.normalization(x[['RelativeHumidity']]))\n",
    "        x_pipe.append(self.normalization(x[['ExhaustVacuumHg']]))\n",
    "        x_pipe.append(self.standardization(x[['TemperatureCelcius']]))\n",
    "        x_pipe.append(self.standardization(x[['AmbientPressureMillibar']]))\n",
    "        x_pipe.append(self.extractMonth(x[['Datetime']], drop_original=False))\n",
    "        x_pipe.append(self.groupDate(x[['Datetime']]))\n",
    "        \n",
    "        # Join transformed features & drop original features\n",
    "        self.x_train_transformed = pd.concat(x_pipe, axis=1).drop(self.drop_feature, axis=1)\n",
    "        \n",
    "        return(self.x_train_transformed)\n",
    "    \n",
    "    # Apply the fitted transformation pipeline to new dataset\n",
    "    def transform(self, x):\n",
    "        x = x.copy()\n",
    "        x_pipe = [x]\n",
    "        for i in range(len(self.transformations)):\n",
    "            x_pipe.append(self.transformations[i](x[[self.transform_feature[i]]], self.transform_params[i]))        \n",
    "        return(pd.concat(x_pipe, axis=1).drop(self.drop_feature, axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform data\n",
    "x_transform = TransformPipe(x_train)\n",
    "x_train_transformed = x_transform.fit() # Fit to training set\n",
    "x_valid_transformed = x_transform.transform(x_valid) # Apply same transformation to validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid_transformed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Model Building\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using **Scikit-Learn** our choice of machine learning tools. \n",
    "\n",
    "To demonstrate the influence of feature engineering, we will fit to two different regressor models from Scikit-Learn:\n",
    "1. Support Vector Regressor\n",
    "2. Decision Tree Regressor\n",
    "\n",
    "The support vector model is a distance-algorithm based model. Hence, standardizations and normalization of numerical features often leads to increase performances. Whereas, a decision tree will be less dependent on normalized/standardized numerica models.\n",
    "\n",
    "To learn more about different machine learning models, the [Scikit-Learn User Guide](https://scikit-learn.org/stable/user_guide.html) provides a great references, and detailed documentations about the hyperparameters associated with each model. Another great read for introductions to machine learning models is [All Machine Learning Models Explained in 6 Minutes](https://towardsdatascience.com/all-machine-learning-models-explained-in-6-minutes-9fe30ff6776a) by Terence Shin. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = svm.SVR()\n",
    "print(f'SVR Model with Transformed Data - Cross-validation Score: {cross_val_score(svr, x_train_transformed, np.ravel(y_train), cv=kf).mean()}')\n",
    "print(f'SVR Model without Transformed Data - Cross-validation Score: {cross_val_score(svr, x_train.drop(columns=\"Datetime\"), np.ravel(y_train), cv=kf).mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**\n",
    ">\n",
    "> The difference in performance on the transformed dataset. After applying standardization and normalizations, the scoring of our model increased significantly. Since we have not applied any hyperparameter tuning, this performance gain is straightly from the feature engineering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = tree.DecisionTreeRegressor(random_state=123)\n",
    "print(f'Decision Tree Model with Transformed Data - Cross-validation Score: {cross_val_score(dtree, x_train_transformed, np.ravel(y_train), cv=kf).mean()}')\n",
    "print(f'Decition Tree Model without Transformed Data - Cross-validation Score: {cross_val_score(dtree, x_train.drop(columns=\"Datetime\"), np.ravel(y_train), cv=kf).mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Score Summary\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "**Score**\n",
    "\n",
    "| Model | Untransformed | Transformed|\n",
    "|---|---|---|\n",
    "| SVR | 0.32 | 0.92 |\n",
    "|Decision Tree| 0.92 | 0.92 |\n",
    "\n",
    "\n",
    "As the SVR Model on the transformed data gives us the best test score. We will choose the support-vector regressor as the final model to perform validations with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "The purpose of hyperparameter tuning to to ensure your machine learning model fits your data in the best way possible. Hyperparameters are the rules and framework in how your model fits the data. By adjusting the hyperparameters, we can optimize the model fitting and adjust for the bias-variance balance. It is important to distinguish the difference between the terms hyperparameters and parameters, especially since they are not used consistently across different fields. \n",
    "\n",
    "> In machine learning, \"[model] parameters are estimated from data automatically and model hyperparameters are set manually and are used in processes to help estimate model parameters. [However, model] hyperparameters are sometimes also referred to as parameters because they are the parts of the machine learning that must be set manually and tuned.\" -- Quoted from [What is the Difference Between a Parameter and a Hyperparameter? By Jason Brownlee](https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/#:~:text=In%20summary%2C%20model%20parameters%20are,be%20set%20manually%20and%20tuned.)\n",
    "\n",
    "For example, if your model is a decision tree. Hyperparameters that can be set manually includes:\n",
    "- The max depth of the tree\n",
    "- Can a feature can be used more than once in the decision stumps\n",
    "- etc. \n",
    "\n",
    "Whereas the parameterst the the model fits automatically includes:\n",
    "- Which feature to use at a particular stump\n",
    "- What values to make the decision on\n",
    "- etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "\n",
    "We will be using Scikit-Learn as our choice of machine learning tools. As seen above, the support vector regressor (SVR) model gave the best performance on our transformed dataset, thus we will be using SVR to continue introducing the hyperparameter tuning process.\n",
    "\n",
    "For demonstration, we will tune the SVR models on the following hyperparameters:\n",
    "1. Kernel: The function used to classify and regress. Possible values are categorical ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "2. C: The regularization parameter. The lower the value, the more generalized the model is. Possible values are positive float values. \n",
    "3. Gamma: Kernel coefficient for 'rbf', 'poly' and 'sigmoid'. Scikit-learn has an 'auto' and 'scale' mode where Gamma is dependent on the number of features and/or variance of X. However, we will be setting the float values in this excercise. Possible values are floats. \n",
    "\n",
    "> **FYI** To learn more about different machine learning models, the [Scikit-Learn User Guide](https://scikit-learn.org/stable/user_guide.html) provides a great references, and detailed documentations about the hyperparameters associated with each model. Another great read for introductions to machine learning models is [All Machine Learning Models Explained in 6 Minutes](https://towardsdatascience.com/all-machine-learning-models-explained-in-6-minutes-9fe30ff6776a) by Terence Shin. \n",
    "\n",
    "There are two common mode of hyperparameter tuning:\n",
    "1. Grid Search: With given lists/limits of hyperparameters, the model is trained and scored for each combination of hyperparameters exhaustively.\n",
    "2. Random Search: With given lists/limits/distribution of hyperparameters, the model is trained for a specific iteration on randomly sampled hyperparameters from the defined space. \n",
    "\n",
    "> **Side Note**: Another method to hyperparamenter tuning is to use Bayesian Optimization. However due to the scope of the topic, we will not be covering Bayesian optimization in this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search CV\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "In a grid search, we difine a list of values for each hyperparamenter, and an exhaustive search is executed to find the combination of values that returns the best results. For every single combination of hyperparameters, a k-fold cross validation is used to score the model performance. \n",
    "\n",
    "**Pros**:\n",
    "- With cross-validation to help levitate optimization bias, grid search is often thought to guarantee finding the best performing model with the given the hyperparameter option. \n",
    "\n",
    "**Cons**:\n",
    "- Inefficient if the hyperparameter space to explore is vase\n",
    "- Computation power and time increases exponentially with additional hyperparameter values to explore\n",
    "- If different parameters weight differently, there is a chance that the grid search misses the area in which the model performs the best. \n",
    "\n",
    "Picture from [Bergstra, 2012](http://jmlr.csail.mit.edu/papers/volume13/bergstra12a/bergstra12a.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='img/grid_vs_random.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of demonstration, we will run grid search on a small number of hyperparameter values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example of hyperparameter values lists. \n",
    "# # In this setup, we have 45 models to evaluation. With a 5-folds CV, the grid search builds a total of 225 models\n",
    "\n",
    "# kernel = ['linear', 'rbf', 'poly']\n",
    "# C = [0.01, 0.1, 1, 10, 100]\n",
    "# gamma = [0.01, 0.1, 1]\n",
    "\n",
    "\n",
    "kernel = ['rbf', 'linear']\n",
    "C = [0.1, 1]\n",
    "gamma = [0.1, 1]\n",
    "\n",
    "param_grid = dict(kernel=kernel,\n",
    "                  C=C,\n",
    "                  gamma=gamma)\n",
    "\n",
    "grid = GridSearchCV(estimator=svr,\n",
    "                    param_grid=param_grid,\n",
    "                    scoring='r2',\n",
    "                    verbose=1,\n",
    "                    n_jobs=-1,\n",
    "                    cv=5,\n",
    "                    refit=True)\n",
    "\n",
    "grid_result = grid.fit(x_train_transformed, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Score: ', grid_result.best_score_)\n",
    "print('Best Params: ', grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>If we were to run the grid search on the commented parameters\n",
    "\n",
    "```\n",
    "kernel = ['linear', 'rbf', 'poly']\n",
    "C = [0.01, 0.1, 1, 10, 100]\n",
    "gamma = [0.01, 0.1, 1]\n",
    "```\n",
    "\n",
    "GridSearchCV with 5-folds CV evaluated a total of 225 model, and took a the total run time of 5 minutes. \n",
    "The model that returned the best score is as following:\n",
    "\n",
    "```\n",
    "Best Score:  0.9321263237332635\n",
    "Best Params:  {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n",
    "```\n",
    "\n",
    "> *Note that the score may deviate slightly due to the random shuffling of samples when doing cross validation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized Search CV\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "In randomized search CV, we provide the distribution of hyperparameter instead of a discrete list. In the randomize search, we can specify how many random models that we wish to evaluate. Every single model evaluation will sample a hyperparameter value from the given distribution. In Scikit-Learn RandomizedSearchCV, we can also provide a discrete list of values, in which the algorithm will sample from the discrete list. \n",
    "\n",
    "**Pros**\n",
    "- Can control operation time and limit the number of models to evaluate\n",
    "- Efficient in exploring a vast hyperparameter space\n",
    "- Often used to learn more about what hyperparameter is important to your data\n",
    "\n",
    "**Cons**\n",
    "- There is a random chance that we can miss the best hyperparameter space\n",
    "- Defining the distribution of hyperparameter is a bit of an art. Defining a poor hyperparameter distribution can lead to long run time as models failed to reach convergence. \n",
    "\n",
    "For the purpose of demonstration, we will run a randomized search with 8 iterations and 5-folds CV. Which equals to a total of 40 models built in the process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = ['linear', 'rbf']\n",
    "C = stats.loguniform(a=0.000001, b=500)\n",
    "gamma = stats.loguniform(a=0.000001,b=10)\n",
    "\n",
    "param_grid = dict(kernel=kernel,\n",
    "                  C=C,\n",
    "                  gamma=gamma)\n",
    "\n",
    "# svr = svm.SVR()\n",
    "rand = RandomizedSearchCV(estimator=svr,\n",
    "                    param_distributions=param_grid,\n",
    "                    scoring='r2',\n",
    "                    verbose=1,\n",
    "                    n_jobs=-1,\n",
    "                    cv=5,\n",
    "                    n_iter=8,\n",
    "                    refit=True)\n",
    "\n",
    "rand_result = rand.fit(x_train_transformed, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best Score: ', rand_result.best_score_)\n",
    "print('Best Params: ', rand_result.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>In a previous run of the randomized search with 100 iterations, it took a the total run time of 11 minutes. \n",
    "The model that returned the best score is as following:\n",
    "\n",
    "```\n",
    "Best Score:  0.931420243466335\n",
    "Best Params:  {'C': 47.201216019420606, 'gamma': 0.015386809496127108, 'kernel': 'rbf'}\n",
    "```\n",
    "\n",
    "> *Note that the score and parameters will deviate in repeated runs due to the random nature of the sampling process*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Hyperparameter Space\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n",
    "\n",
    "To continue with the reminder of the notebook, we will use the results from a RandomizedSearchCV with 100 iterations and 5-folds CV.\n",
    "\n",
    "```\n",
    "kernel = ['linear', 'rbf', 'poly']\n",
    "C = stats.loguniform(a=0.000001, b=500)\n",
    "gamma = stats.loguniform(a=0.000001,b=10)\n",
    "\n",
    "param_grid = dict(kernel=kernel,\n",
    "                  C=C,\n",
    "                  gamma=gamma)\n",
    "\n",
    "svr = svm.SVR(max_iter=5000)\n",
    "rand = RandomizedSearchCV(estimator=svr,\n",
    "                    param_distributions=param_grid,\n",
    "                    scoring='r2',\n",
    "                    verbose=1,\n",
    "                    n_jobs=-1,\n",
    "                    cv=5,\n",
    "                    n_iter=100,\n",
    "                    refit=True)\n",
    "\n",
    "rand_result = rand.fit(x_train_transformed, y_train_array)\n",
    "\n",
    "cv_results = pd.DataFrame(rand_result.cv_results_)\n",
    "cv_results.to_csv('randSearchCVResults.csv', index=False)\n",
    "```\n",
    "\n",
    "The result summary was extracted and saved in a separate file in which we will load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.read_csv('data/randSearchCVResults.csv')\n",
    "scores = cv_results.set_index('rank_test_score').sort_index()[['mean_fit_time', 'param_C', 'param_gamma', 'param_kernel', 'mean_test_score']]\n",
    "\n",
    "scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3, figsize=(30,5))\n",
    "cmaps = ['Greens', 'Reds_r']\n",
    "for i, k in enumerate(['linear', 'rbf', 'poly']):\n",
    "    sub_df = scores[scores.param_kernel==k]\n",
    "    gr = ax[i].scatter(x=sub_df[sub_df.mean_test_score>0]['param_C'], y=sub_df[sub_df.mean_test_score>0]['param_gamma'], c=sub_df[sub_df.mean_test_score>0]['mean_test_score'], cmap='Greens', s=50, vmin=0, vmax=1, edgecolors='b')\n",
    "    re = ax[i].scatter(x=sub_df[sub_df.mean_test_score<0]['param_C'], y=sub_df[sub_df.mean_test_score<0]['param_gamma'], c=sub_df[sub_df.mean_test_score<0]['mean_test_score'], cmap='Reds_r', s=50, vmax=0, vmin=scores.mean_test_score.min(), edgecolors='b')\n",
    "    ax[i].set_yscale('log')\n",
    "    ax[i].set_xscale('log')\n",
    "    ax[i].set_title('Kernel: '+k)\n",
    "    ax[i].set_xlabel('Hyparameter: C')\n",
    "    ax[i].set_ylabel('Hyparameter: Gamma')\n",
    "\n",
    "divider = make_axes_locatable(ax[2])\n",
    "ax2 = divider.append_axes('right', size='2%', pad=0.5)\n",
    "ax2.text(-1.5, 1.01, 'R2 Score', fontweight='bold')\n",
    "fig.canvas.draw()\n",
    "pos = ax2.get_position()\n",
    "\n",
    "width = 0.005\n",
    "ax3 = fig.add_axes([pos.xmin, 0.5*(pos.ymax-pos.ymin), width,  0.48])\n",
    "fig.colorbar(gr, cax=ax3)\n",
    "ax4 = fig.add_axes([pos.xmin, pos.ymin, width,  0.5*(pos.ymax-pos.ymin)])\n",
    "fig.colorbar(re, cax=ax4)\n",
    "ax2.set_yticks([])\n",
    "ax2.set_xticks([])\n",
    "ax2.set_facecolor((1, 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Insights**\n",
    ">\n",
    "> - In the graph above we see that the model that gives the best score are with linear and RBF kernels. Whereas the polynomial kernel tends to give poor results. \n",
    "> - We can also see that in the SVR model, the hyperparameter C plays a more important role than Gamma.\n",
    "> - We can also then narrow our hyperparameter tuning to the specific area that gives high scores\n",
    ">  - Linear kernel with C ranging from 0.001 to 100\n",
    ">  - RBF kernal with C ranging from 0.1 to 100\n",
    ">\n",
    "> It is very common to first start with a randomize search in at the start of the ML process, then slowly narrow the range of hyperparameters to explore until we come to a model that we are satisfied with. For ML applications where high accuracy and precision is required, grid search would be used to explore in a fine-grain scale for the best model after a more specified range is identified with randomized search. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Model Evaluations\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Evaluation\n",
    "\n",
    "As we have done so far in the notebook, the most basic type of evaluation would be to compare the test CV score the performance of the models on the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = svm.SVR()\n",
    "base_model.fit(x_train_transformed, np.ravel(y_train))\n",
    "cv_results_base = cross_validate(svr, x_train_transformed, np.ravel(y_train), cv=5, scoring='r2', return_train_score=True)\n",
    "print('BASE SVR MODEL:')\n",
    "print('CV Train Score:', cv_results_base['train_score'].mean())\n",
    "print('CV Test Score:', cv_results_base['test_score'].mean())\n",
    "print('\\n')\n",
    "\n",
    "best_model = svm.SVR(kernel = 'rbf', C=47.201216, gamma = 0.015387)\n",
    "best_model.fit(x_train_transformed, np.ravel(y_train))\n",
    "cv_results_best = cross_validate(best_model, x_train_transformed, np.ravel(y_train), cv=5, scoring='r2', return_train_score=True)\n",
    "print('OPTIMIZED SVR MODEL')\n",
    "print('CV Train Score:', cv_results_best['train_score'].mean())\n",
    "print('CV Test Score:', cv_results_best['test_score'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Evaluation\n",
    "\n",
    "However, there are also statistical models that can help us evaluate whether a model is performing better than others. One such statistical test is the Wilcoxon signed-rank test, which uses the concept of k-fold cross validations to generate a sample of scores for each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='img/wilconxon-test.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The picture is retrieved from a Medium article [Validating your Machine Learning Model by Maarten Grootendorst](https://towardsdatascience.com/validating-your-machine-learning-model-25b4c8643fb7), which is also a great read on other model comparison tests such as the McNemar’s Test, and the 5x2CV paired t-test. It also provide in-depth explanation of different cross-validation methods and their appropriate uses on different types of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results for each model on the same folds\n",
    "results_base_model = cross_val_score(base_model, x_train_transformed, np.ravel(y_train), cv=kf)\n",
    "results_best_model = cross_val_score(best_model, x_train_transformed, np.ravel(y_train), cv=kf)\n",
    "stat, p = stats.wilcoxon(results_base_model, results_best_model, zero_method='zsplit');\n",
    "print('The Wilcoxon signed-rank test p-value is ', p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate the p-value of the statistical test. If the p-value is  lower than 0.05, we reject the null hypothesis that there are no significant differences between the models. In this case our model performance is significantly different from each other. In combination with the performance we see on the validation test, we can conclude that our optimized model after hyperparameter tuning is performing better than the base model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Validations\n",
    "\n",
    "We validate our model on the validation dataset. The models had not seen this dataset throughout the building process, thus this would be the ultimiate evalution to ensure that the model is generalizable to new data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_score = base_model.score(x_valid_transformed, np.ravel(y_valid))\n",
    "print('Base model validation Score: ', base_score)\n",
    "regr_score = best_model.score(x_valid_transformed, np.ravel(y_valid))\n",
    "print('Optimized model validation Score: ', regr_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residuals Evaluations\n",
    "\n",
    "Another common technique in evaluating the model fit is to analysis the residuals. Residuals are the difference between the predicted values and the expected values. A good performing model should have residuals clustered around 0 with no significant trends. We will be taking a look into the residuals from the results of the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid['PredictedEnergy'] = best_model.predict(x_valid_transformed)\n",
    "valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcuate Residuals\n",
    "valid = valid.assign(Residuals=valid.PredictedEnergy-valid.HourlyEnergyOutputMW)\n",
    "\n",
    "# Plot\n",
    "px.scatter(valid, x = 'PredictedEnergy', y='Residuals',\n",
    "          labels={'PredictedEnergy':'Fitted Values'},\n",
    "          title = 'Fitted vs. Residuals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another method is to look at the histogram of the residuals and the QQ-plot. For the histogram, we should see a relatively normal distribution around 0, and as for the QQ-plot, we expect the residuals to lay close to the expected line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Stats for QQ-Plots\n",
    "normed_residuals = (valid.Residuals-valid.Residuals.mean())/valid.Residuals.std()\n",
    "qq = stats.probplot(normed_residuals, dist='norm', sparams=(1))\n",
    "qq_array = np.array([qq[0][0][0], qq[0][0][-1]])\n",
    "qq_array\n",
    "\n",
    "# Plotting\n",
    "fig = make_subplots(rows=1, cols=2, subplot_titles=['Histogram of Normalized Residuals', 'QQ-Plot of Normalized Residuals'])\n",
    "\n",
    "\n",
    "fig.add_trace(go.Scatter(x=qq[0][0], y=qq[0][1], mode='markers'),row=1, col=2)\n",
    "fig.add_trace(go.Scatter(x=qq_array, y=qq[1][1] + qq[1][0]*qq_array, mode='lines'),row=1, col=2)\n",
    "fig.add_trace(go.Histogram(x=normed_residuals),row=1, col=1)\n",
    "\n",
    "fig.update_xaxes(title_text=\"Normalized Residuals\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Theoritecal Quantiles\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Counts\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Sample Quantiles\", row=1, col=1)\n",
    "\n",
    "\n",
    "fig.layout.update(showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In looking at the residual plots, we can see that there are three predictions that are very far off from the expected value. These may be areas in which we can look into the dataset to investigate whether these observations are extreme values, or else why the model is performing poorly on these values. However overall, the residual plots looks like the model is performing well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted Vs. Expected Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data for Plotting\n",
    "df_plot = pd.melt(valid, id_vars=['Datetime'], value_vars=['HourlyEnergyOutputMW', 'PredictedEnergy'],var_name='ActvsPred', value_name='HourlyEnergyOutput')\\\n",
    "            .replace({'HourlyEnergyOutputMW': 'Actual', 'PredictedEnergy': 'Predicted'})\n",
    "df_plot.Datetime = pd.to_datetime(df_plot.Datetime)\n",
    "\n",
    "# Plot\n",
    "fig = px.line(df_plot, \n",
    "        x='Datetime', y='HourlyEnergyOutput', color = 'ActvsPred',\n",
    "        title='Energy Output Actual Vs. Prediction | Validation Score: %.2f' %(regr_score),\n",
    "        labels={\n",
    "             \"HourlyEnergyOutput\": \"Energy Output (MW)\",\n",
    "             \"ActvsPred\": \"Value Type\"\n",
    "        },)\n",
    "\n",
    "fig.update_layout(\n",
    "    legend=dict(\n",
    "        orientation=\"h\",\n",
    "        yanchor=\"bottom\",\n",
    "        y=1.02,\n",
    "        xanchor=\"left\",\n",
    "        x=0\n",
    "    ))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Market Analysis\n",
    "\n",
    "[[ go back to the top ]](#Table-of-contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Energy Output from Sensor Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the last week's worth of data that we reserve with removed energy output information? We will now use that to imitate \"real-time\" sensory data that we might recieve from the powerplant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>We will use the same machine learning model to predict eneryg output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Transform data\n",
    "forecast_transformed = x_transform.transform(forecast)\n",
    "\n",
    "# # 2. Predict\n",
    "forecast['OutputEnergyPredictedMW'] = best_model.predict(forecast_transformed)\n",
    "forecast.Datetime = pd.to_datetime(forecast.Datetime)\n",
    "forecast.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>Lastly, we will join our prediction data with the market forecast data from AESO using the datetime as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set datetime as index\n",
    "df_output_indexed = forecast.set_index('Datetime')\n",
    "aeso_df_indexed = aeso_df.set_index('Datetime')\n",
    "\n",
    "# Join and subset data\n",
    "df_market = df_output_indexed.join(aeso_df_indexed)\n",
    "df_market = df_market[['OutputEnergyPredictedMW', 'Forecast Pool Price']]\n",
    "df_market.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate Internal Load from EnergyOutput data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this demonstration, we will use the distribution of 6 month's record of energy production from the powerplant to randomly generate an internal load requirement. This will be the internal demand for the powerplant's company that needs to be satisfied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=1)\n",
    "energy_dist_params = stats.norm.fit(df['HourlyEnergyOutputMW'])\n",
    "df_market.loc[:,'InternalDemandMW'] = stats.norm(*energy_dist_params).rvs(df_market.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_market.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate Cost and Recommend Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to  U.S. Energy Information Administration (EIA) 2020 Report. The average  levelized cost of electricity (LCOE) of combined cycle powerplant is 36.61 $/MWh. With price data from AESO, we can conduct an analysis to provide recommended actions based on market forecasts. \n",
    "\n",
    "The logic that generates the recommended actions are as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('img/market-logic.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_market.loc[:,'PredictedExcessMW'] = df_market.OutputEnergyPredictedMW - df_market.InternalDemandMW\n",
    "df_market.loc[:, 'Shortage/Surplus'] = np.where(df_market.PredictedExcessMW >= 0, 'surplus', 'shortage')\n",
    "\n",
    "df_market.loc[:, 'EstimatedFullProductionCostDollar'] = df_market.OutputEnergyPredictedMW*36.61\n",
    "df_market.loc[:, 'BuyDemandFromGridDollar'] = df_market['InternalDemandMW']*df_market['Forecast Pool Price']\n",
    "\n",
    "df_market.loc[:, 'BuyShortageFromGridDollar'] = np.where(df_market['Shortage/Surplus']=='shortage', \n",
    "                                                         abs(df_market['PredictedExcessMW'])*df_market['Forecast Pool Price'], \n",
    "                                                         None)\n",
    "df_market.loc[:, 'SellSurplusToGridDollar'] = np.where(df_market['Shortage/Surplus']=='surplus', \n",
    "                                                       abs(df_market['PredictedExcessMW'])*df_market['Forecast Pool Price'], \n",
    "                                                       None)\n",
    "\n",
    "df_market.loc[:, 'RecommendAction'] = np.where(df_market['Forecast Pool Price'] < 36.61, 'Buy Load',\n",
    "                                               np.where(df_market['Shortage/Surplus']=='shortage', 'Produce & Buy Shortage', 'Produce & Sell Surplus'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_market.loc[:, 'EstimateNetCost'] = np.where(df_market['RecommendAction']=='Buy Load', \n",
    "                                               df_market['BuyDemandFromGridDollar'],\n",
    "                                               np.where(df_market['RecommendAction']=='Produce & Buy Shortage',\n",
    "                                                       df_market['EstimatedFullProductionCostDollar']+df_market['BuyShortageFromGridDollar'],\n",
    "                                                       df_market['EstimatedFullProductionCostDollar']-df_market['SellSurplusToGridDollar']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_market.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1, figsize = (20, 10))\n",
    "ax[0].plot(df_market.index, df_market['PredictedExcessMW'], color = 'dimgrey')\n",
    "ax[0].axhspan(ymin=0, ymax=80, facecolor='palegreen', alpha=0.2)\n",
    "ax[0].axhspan(ymin=-80, ymax=0, facecolor='lightcoral', alpha=0.2)\n",
    "ax[0].axhline(color='r', linestyle='--')\n",
    "ax[0].set_xlabel(\"\\nDatetime\")\n",
    "ax[0].set_ylabel('Energy (MW)\\n')\n",
    "ax[0].set_title('Energy Production Shortage/Surplus')\n",
    "\n",
    "groups = df_market.groupby('RecommendAction')\n",
    "ax[1].plot(df_market.index, df_market['EstimateNetCost'], c='lightgrey')\n",
    "for name, group in groups:\n",
    "    ax[1].plot(group.index, group.EstimateNetCost, marker='o', linestyle='', label=name)\n",
    "ax[1].legend(title=\"Recommended Action\")\n",
    "ax[1].set_xlabel(\"\\nDatetime\")\n",
    "ax[1].set_ylabel('Estimated Net Cost ($)\\n')\n",
    "ax[1].set_title('Estimated Net Cost & Actions\\n')\n",
    "plt.tight_layout(h_pad=5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your database URI by executing 'printenv | grep DB_*' in terminal \n",
    "sqlalchemy_uri = \"postgres://user:password@0.0.0.0:5432/default\"\n",
    "\n",
    "# OR retrieve it with your environmental variable. Replace \"DB_POWERPLANT_ENERGY_PREDICTION_URI\" with your environmental variable name\n",
    "sqlalchemy_uri = os.environ['DB_POWERPLANT_ENERGY_PREDICTION_URI']  \n",
    "\n",
    "engine = sqlalchemy.create_engine(sqlalchemy_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sql\n",
    "%sql $sqlalchemy_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_market = df_market.reset_index(level=0)\n",
    "df_market['Datetime'] = pd.to_datetime(df_market.Datetime)\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    # Other application can only access data tables created by the owner/creator\n",
    "    conn.execute('SET ROLE default_data_owner;')\n",
    "    \n",
    "    try :\n",
    "        # Push the Data to the Database\n",
    "        df_market.to_sql('marketpredictions', con=conn, if_exists='replace', index = False)\n",
    "    \n",
    "    except:\n",
    "        # Remove the special characters from the Column Names\n",
    "        df_market.columns = list(map(lambda x: re.sub('[^A-Za-z0-9]+', '', x),df.columns ))\n",
    "        \n",
    "        # Push the Data to the Database\n",
    "        df_market.to_sql('marketpredictions', con=conn, if_exists='replace', index = False)\n",
    "        \n",
    "        print('Success\\nPlease Note: The special characters in the column names have been removed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast.Datetime = pd.to_datetime(forecast.Datetime)\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    # Other application can only access data tables created by the owner/creator\n",
    "    conn.execute('SET ROLE default_data_owner;')\n",
    "    \n",
    "    try :\n",
    "        # Push the Data to the Database\n",
    "        forecast.to_sql('energypredictions', con=conn, if_exists='replace', index = False)\n",
    "    \n",
    "    except:\n",
    "        # Remove the special characters from the Column Names\n",
    "        forecast.columns = list(map(lambda x: re.sub('[^A-Za-z0-9]+', '', x),df.columns ))\n",
    "        \n",
    "        # Push the Data to the Database\n",
    "        forecast.to_sql('energypredictions', con=conn, if_exists='replace', index = False)\n",
    "        \n",
    "        print('Success\\nPlease Note: The special characters in the column names have been removed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid.Datetime = pd.to_datetime(valid.Datetime)\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    # Other application can only access data tables created by the owner/creator\n",
    "    conn.execute('SET ROLE default_data_owner;')\n",
    "    \n",
    "    try :\n",
    "        # Push the Data to the Database\n",
    "        valid.to_sql('mlvalidations', con=conn, if_exists='replace', index = False)\n",
    "    \n",
    "    except:\n",
    "        # Remove the special characters from the Column Names\n",
    "        valid.columns = list(map(lambda x: re.sub('[^A-Za-z0-9]+', '', x),df.columns ))\n",
    "        \n",
    "        # Push the Data to the Database\n",
    "        valid.to_sql('mlvalidations', con=conn, if_exists='replace', index = False)\n",
    "        \n",
    "        print('Success\\nPlease Note: The special characters in the column names have been removed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql \\dt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
